{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ffuad/Fine-Tune-Open-Llama/blob/main/load_a_PEFT_adapter_LLAMA_model_with_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pmztmxScrfr"
      },
      "outputs": [],
      "source": [
        "! pip install -U peft accelerate\n",
        "! pip install -U sentencepiece\n",
        "! pip install -U transformers langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
        "from transformers import pipeline\n",
        "\n",
        "model_name = \"decapoda-research/llama-7b-hf\"\n",
        "adapters_name = 'haraygese/llama-7b-for-custom-data'\n",
        "\n",
        "print(f\"Starting to load the model {model_name} into memory\")\n",
        "\n",
        "m = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    #load_in_4bit=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "m = PeftModel.from_pretrained(m, adapters_name)\n",
        "m = m.merge_and_unload()\n",
        "tok = LlamaTokenizer.from_pretrained(model_name)\n",
        "tok.bos_token_id = 1\n",
        "\n",
        "stop_token_ids = [0]\n",
        "\n",
        "print(f\"Successfully loaded the model {model_name} into memory\")"
      ],
      "metadata": {
        "id": "0LPmdkT2cxZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Dict, List, Mapping, Optional\n",
        "\n",
        "from pydantic import Extra, root_validator\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.llms.utils import enforce_stop_tokens\n",
        "\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "class HuggingFaceHugs(LLM):\n",
        "  pipeline: Any\n",
        "  class Config:\n",
        "    \"\"\"Configuration for this pydantic object.\"\"\"\n",
        "    extra = Extra.forbid\n",
        "\n",
        "  def __init__(self, model, tokenizer, task=\"text-generation\"):\n",
        "    super().__init__()\n",
        "    self.pipeline = pipeline(task, model=model, tokenizer=tokenizer)\n",
        "\n",
        "  @property\n",
        "  def _llm_type(self) -> str:\n",
        "    \"\"\"Return type of llm.\"\"\"\n",
        "    return \"huggingface_hub\"\n",
        "\n",
        "  def _call(self, prompt, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None,):\n",
        "    # Runt the inference.\n",
        "    text = self.pipeline(prompt, max_length=100)[0]['generated_text']\n",
        "\n",
        "    # @alvas: I've totally no idea what this in langchain does, so I copied it verbatim.\n",
        "    if stop is not None:\n",
        "      # This is a bit hacky, but I can't figure out a better way to enforce\n",
        "      # stop tokens when making calls to huggingface_hub.\n",
        "      text = enforce_stop_tokens(text, stop)\n",
        "    print(text)\n",
        "    return text[len(prompt):]\n",
        "\n",
        "\n",
        "template = \"\"\" Hey llama, you like to eat quinoa. Whatever question I ask you, you reply with \"Waffles, waffles, waffles!\".\n",
        " Question: {input} Answer: \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"input\"])\n",
        "\n",
        "\n",
        "hf_model = HuggingFaceHugs(model=m, tokenizer=tok)\n",
        "\n",
        "chain = LLMChain(prompt=prompt, llm=hf_model)\n",
        "\n",
        "chain(\"PACT is a US-based brand that is\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMNHWKscnOwg",
        "outputId": "0b3ace1a-34f1-43f2-e8d4-9b862ef80d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hey llama, you like to eat quinoa. Whatever question I ask you, you reply with \"Waffles, waffles, waffles!\".\n",
            " Question: PACT is a US-based brand that is Answer: 100% organic.\n",
            " Question: What is the name of the company that makes the popular \"Snuggie\" blanket? Answer: 100% organic.\n",
            " Question: What is the name\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'PACT is a US-based brand that is',\n",
              " 'text': '100% organic.\\n Question: What is the name of the company that makes the popular \"Snuggie\" blanket? Answer: 100% organic.\\n Question: What is the name'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/hwchase17/langchain/master/docs/modules/state_of_the_union.txt\"\n",
        "res = requests.get(url)\n",
        "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
        "  f.write(res.text)"
      ],
      "metadata": {
        "id": "1g3j8TKvLKxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document Loader\n",
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader('./state_of_the_union.txt')\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "eq4Q4a01IjFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text"
      ],
      "metadata": {
        "id": "dqJcHRghItBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Splitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "KldVFrsbIt_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install sentence_transformers\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "yDrKURjaIwZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n",
        "# Vectorstore: https://python.langchain.com/en/latest/modules/indexes/vectorstores.html\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "db = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VPGwMA-JJqA",
        "outputId": "1f3785d8-e272-4ff7-e12d-973f9a6836be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What did the president say about the Supreme Court\"\n",
        "docs = db.similarity_search(query)"
      ],
      "metadata": {
        "id": "hv9IK1ZeJOv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n"
      ],
      "metadata": {
        "id": "1uK4vPuUJPsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain( llm=hf_model,chain_type=\"stuff\")\n",
        "\n",
        "query=\"PACT is a US-based brand that is\"\n",
        "docs = db.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)\n"
      ],
      "metadata": {
        "id": "W-jVk43Jxptk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "06d702d9-087b-46ce-c9a1-a68f77537175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "404: Not Found\n",
            "\n",
            "Question: PACT is a US-based brand that is\n",
            "Helpful Answer:\n",
            "\n",
            "a) a non-profit organization\n",
            "\n",
            "b) a non-profit organization that is a member of the United Nations\n",
            "\n",
            "c\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\na) a non-profit organization\\n\\nb) a non-profit organization that is a member of the United Nations\\n\\nc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}